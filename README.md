# 大模型入门教程

来自 ：

1. [Lordog/dive-into-llms: 《动手学大模型Dive into LLMs》系列编程实践教程](https://github.com/Lordog/dive-into-llms  ) 
2.  [bbruceyuan/Hands-On-Large-Language-Models-CN: 中文翻译的 Hands-On-Large-Language-Models (hands-on-llms)，动手学习大模型](https://github.com/bbruceyuan/Hands-On-Large-Language-Models-CN) 
3. [面向开发者的LLM入门教程](https://datawhalechina.github.io/llm-cookbook/)

| 章节                                                         |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 第一章: 介绍大模型 / 微调与部署                              | 预训练模型微调与部署指南：想提升预训练模型在指定任务上的性能？让我们选择合适的预训练模型，在特定任务上进行微调，并将微调后的模型部署成方便使用的Demo！ |
| 第二章: Tokens and Embeddings /提示学习与思维链              | 大模型的API调用与推理指南：“AI在线求鼓励？大模型对一些问题的回答令人大跌眼镜，但它可能只是想要一句「鼓励」” |
| 第三章: Looking Inside Transformer LLMs / 知识编辑           | 语言模型的编辑方法和工具：想操控语言模型在对指定知识的记忆？让我们选择合适的编辑方法，对特定知识进行编辑，并将对编辑后的模型进行验证！ |
| 第四章: Text Classification /数学推理                        | 如何让大模型学会数学推理？让我们快速蒸馏一个迷你R1！         |
| 第五章: Text Clustering and Topic Modeling / 模型水印        | 语言模型的文本水印：在语言模型生成的内容中嵌入人类不可见的水印 |
| 第六章: Prompt Engineering / 越狱攻击                        | 想要得到更好的安全，要先从学会攻击开始。让我们了解越狱攻击如何撬开大模型的嘴！ |
| 第七章: Advanced Text Generation Techniques and Tools  / 大模型隐写 | “看不见的墨水”！想让大模型在流畅回答的同时，悄悄携带只有“自己人”能识别的信息吗？大模型隐写告诉你！ |
| 第八章: Semantic Search and Retrieval-Augmented Generation / 多模态模型 | 作为能够更充分模拟真实世界的多模态大语言模型，其如何实现更强大的多模态理解和生成能力？多模态大语言模型是否能够帮助实现AGI？ |
| 第九章: Multimodal Large Language Models / GUI智能体         | 想要饭来张口、解放双手？那么让我们一起来让AI Agent替你点外卖、回消息、购物比价吧！ |
| 第十章: Creating Text Embedding Models / 智能体安全          | 大模型智能体迈向了未来操作系统之旅。然而，大模型在开放智能体场景中能意识到风险威胁吗？ |
| 第十一章: Fine-tuning Representation Models for Classification / RLHF安全对齐 | 基于PPO的RLHF实验指南：本教程”十分危险“，阅读后请检查你的大模型是否在冷笑。 |
| 第 12.1 章: 大模型 SFT                                       |                                                              |
| bonous1 - 动手实现 LoRA（非import peft）                     |                                                              |
| bonus2 从零实现 GRPO (Agent RL），用于 Agentic RAG 训练      |                                                              |